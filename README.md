# LLM API Gateway

Une API Gateway robuste pour les mod√®les de langage (LLM) avec authentification, facturation, fallback automatique et support multi-providers.

## üöÄ Fonctionnalit√©s

- **Multi-providers** : Support d'OpenAI, Anthropic, Google, Meta et plus
- **Authentification flexible** : Cl√©s API personnalis√©es ou tokens Supabase
- **Fallback automatique** : Basculement transparent entre providers en cas d'erreur
- **Streaming** : Support complet du streaming SSE
- **Tool calling** : Support des appels de fonctions
- **Vision** : Support des images dans les requ√™tes
- **Cache intelligent** : Mise en cache des donn√©es pour optimiser les performances
- **Facturation** : Syst√®me de wallet et tracking des co√ªts
- **M√©triques** : Collecte d√©taill√©e des performances
- **Notifications** : Alertes automatiques en cas d'erreur

## üìã Pr√©requis

- Node.js 18+
- PostgreSQL (via Supabase)
- Cl√©s API des providers LLM

## üõ†Ô∏è Installation

1. **Cloner le projet**
```bash
git clone <repository-url>
cd llm-api-gateway
```

2. **Installer les d√©pendances**
```bash
npm install
```

3. **Configuration**
```bash
cp .env.example .env
```

√âditer le fichier `.env` avec vos configurations :

```env
# Supabase Configuration
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key

# Provider API Keys
API_KEY_OPENAI=your_API_KEY_OPENAI
API_KEY_ANTHROPIC=your_API_KEY_ANTHROPIC
API_KEY_GOOGLE=your_API_KEY_GOOGLE

# Server Configuration
PORT=3000
MINIMAL_FUND=0.01
NTFY_ERROR_URL=https://ntfy.makehub.ai/errors
```

4. **Configurer la base de donn√©es**

Ex√©cuter le sch√©ma SQL fourni dans votre instance Supabase pour cr√©er les tables n√©cessaires.

5. **D√©marrer le serveur**
```bash
# D√©veloppement
npm run dev

# Production
npm start
```

## üì° API Endpoints

### Chat Completions
```http
POST /v1/chat/completions
```

Compatible avec l'API OpenAI. Exemples :

**Requ√™te simple :**
```json
{
  "model": "gpt-4o",
  "messages": [
    {"role": "user", "content": "Hello, world!"}
  ]
}
```

**Avec streaming :**
```json
{
  "model": "claude-3-5-sonnet",
  "messages": [
    {"role": "user", "content": "Explain quantum computing"}
  ],
  "stream": true
}
```

**Avec tools :**
```json
{
  "model": "gpt-4o",
  "messages": [
    {"role": "user", "content": "What's the weather in Paris?"}
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get weather information",
        "parameters": {
          "type": "object",
          "properties": {
            "city": {"type": "string"}
          }
        }
      }
    }
  ]
}
```

**Avec images :**
```json
{
  "model": "gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What's in this image?"},
        {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
      ]
    }
  ]
}
```

### Autres endpoints

```http
GET /v1/chat/models          # Liste des mod√®les disponibles
GET /v1/chat/health          # Sant√© des providers
POST /v1/chat/estimate       # Estimation de co√ªt
GET /health                  # Sant√© g√©n√©rale du service
```

## üîê Authentification

### M√©thode 1 : Cl√© API personnalis√©e
```http
X-API-Key: your-custom-api-key
```

### M√©thode 2 : Token Supabase
```http
Authorization: Bearer your-supabase-jwt-token
```

## üèóÔ∏è Architecture

```
src/
‚îú‚îÄ‚îÄ config/          # Configuration (DB, cache)
‚îú‚îÄ‚îÄ middleware/      # Authentification, validation
‚îú‚îÄ‚îÄ providers/       # Impl√©mentations des providers LLM
‚îú‚îÄ‚îÄ services/        # Logique m√©tier
‚îú‚îÄ‚îÄ routes/          # Endpoints API
‚îî‚îÄ‚îÄ index.js         # Point d'entr√©e
```

### Providers support√©s

- **OpenAI** : GPT-4o, GPT-4o-mini
- **Anthropic** : Claude 3.5 Sonnet, Claude 3.5 Haiku
- **Google** : Gemini 1.5 Pro, Gemini 1.5 Flash
- **Meta** : Llama 3.1 (via Together AI)

### Logique de fallback

1. S√©lection des providers compatibles selon le mod√®le demand√©
2. Tri par priorit√© (pr√©f√©rences utilisateur, co√ªt, performance)
3. Tentative sur chaque provider jusqu'au succ√®s
4. Notification des erreurs techniques
5. Retour des erreurs m√©tier directement

## üí∞ Syst√®me de facturation

- **Wallet** : Solde par utilisateur
- **Transactions** : Historique des d√©bits/cr√©dits
- **Estimation** : Calcul du co√ªt avant ex√©cution
- **Tracking** : Mesure pr√©cise des tokens utilis√©s

## üìä M√©triques collect√©es

- **Performance** : Latence, throughput, temps de r√©ponse
- **Usage** : Tokens d'entr√©e/sortie, co√ªts
- **Fiabilit√©** : Taux de succ√®s par provider
- **Streaming** : Time to first chunk, dur√©e totale

## üîß Configuration avanc√©e

### Cache
```env
CACHE_TTL_SECONDS=300
BALANCE_CACHE_TTL_SECONDS=60
```

### Rate limiting
```env
RATE_LIMIT_REQUESTS_PER_MINUTE=60
```

### Providers personnalis√©s

Ajouter un nouveau provider :

```javascript
import { BaseProvider } from './providers/base.js';

class CustomProvider extends BaseProvider {
  transformRequest(request) { /* ... */ }
  transformResponse(response) { /* ... */ }
  // ... autres m√©thodes
}
```

## üöÄ D√©ploiement

### Avec Holo (recommand√©)
```bash
# Configuration pour Holo
npm run build
holo deploy
```

### Avec Docker
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY src ./src
EXPOSE 3000
CMD ["npm", "start"]
```

### Variables d'environnement de production

```env
NODE_ENV=production
PORT=3000
# ... autres variables
```

## üîç Monitoring

### Health checks
```bash
curl http://localhost:3000/health
```

### Logs
Les logs incluent :
- Requ√™tes et r√©ponses
- Erreurs par provider
- M√©triques de performance
- √âv√©nements de cache

### Notifications d'erreur
Configuration ntfy pour les alertes :
```env
NTFY_ERROR_URL=https://ntfy.makehub.ai/errors
```

## üß™ Tests

```bash
# Tests unitaires
npm test

# Test de l'API
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{"model":"gpt-4o","messages":[{"role":"user","content":"Hello!"}]}'
```

## ü§ù Contribution

1. Fork le projet
2. Cr√©er une branche feature
3. Commit les changements
4. Push vers la branche
5. Ouvrir une Pull Request

## üìÑ Licence

MIT License - voir le fichier LICENSE pour plus de d√©tails.

## üÜò Support

- **Issues** : Utiliser GitHub Issues
- **Documentation** : Voir le wiki du projet
- **Contact** : [votre-email]

---

**Note** : Ce projet est con√ßu pour √™tre d√©ploy√© avec Holo mais peut fonctionner sur n'importe quelle plateforme Node.js.
